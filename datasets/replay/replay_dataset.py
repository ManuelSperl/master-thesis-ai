# replay_dataset.py

import numpy as np
import random
from tqdm import tqdm

# ensure the module is re-imported after changes
import importlib

import datasets.dataset_utils
importlib.reload(datasets.dataset_utils)

from datasets.dataset_utils import save_dataset

def generate_replay_dataset_seaquest(env, models, num_episodes=100, perturbation=False, perturbation_level=0.05, save_path='', file_name=''):
    """
    Generates a replay dataset by simulating data collection during training with multiple policies.
    This dataset consists of samples generated by various policies across different stages of training.

    :param env: Gym or VecEnv environment object, pre-initialized (Seaquest environment)
    :param models: list of models (e.g., different stages of the policy) to simulate multiple policies during training
    :param num_episodes: number of episodes to simulate for generating the dataset (Default is 100)
    :param perturbation: bool, whether to add random perturbation to actions
    :param perturbation_level: float, maximum proportion of actions to perturb (default is 0.05, or 5%)
    :param save_path: str, path to save the dataset
    :param file_name: str, name of the file to save the dataset

    :return: list of tuples, each containing a transition (state, action, reward, next_state, done_flag)
    """
    tqdm.write('Generating Replay Dataset for Seaquest...')

    replay_dataset = []  # list to store the dataset
    perturbed_count = 0  # track the number of perturbed steps
    num_models = len(models)

    for episode in tqdm(range(num_episodes), desc='Generating dataset'):
        obs, _ = env.reset()

        done = False
        while not done:
            # Select the model based on the episode number to simulate training progression
            model_index = episode % num_models
            model = models[model_index]

            # Predict action using the selected model
            action, _ = model.predict(obs, deterministic=True)
            action = int(action)  # Ensure action is an integer

            # Perturb action if enabled
            if perturbation and random.random() < perturbation_level:
                action = env.action_space.sample()
                perturbed_count += 1

            step_return = env.step(action)
            new_obs, reward, done = step_return[:3]

            # Append to dataset
            replay_dataset.append((obs, action, reward, new_obs, done))

            obs = new_obs  # Update current state

    print('Length of replay dataset:', len(replay_dataset))
    print(f'Number of perturbed steps: {perturbed_count}')

    env.close()

    # Save the dataset
    save_dataset(replay_dataset, save_path, file_name)

    return replay_dataset
