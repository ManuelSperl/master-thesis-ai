{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Agents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import gc  # Import garbage collector module\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure the module is re-imported after changes\n",
    "import importlib\n",
    "\n",
    "import datasets.dataset_utils\n",
    "importlib.reload(datasets.dataset_utils)\n",
    "\n",
    "from datasets.dataset_utils import set_all_seeds, create_environment, load_dataset, preprocess_and_split, create_dataloaders, inspect_dataset_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure the module is re-imported after changes\n",
    "import importlib\n",
    "\n",
    "import agent_methods.behavioral_cloning_bc.bc_utils\n",
    "importlib.reload(agent_methods.behavioral_cloning_bc.bc_utils)\n",
    "\n",
    "from agent_methods.behavioral_cloning_bc.bc_utils import train_and_evaluate_BC\n",
    "\n",
    "import agent_methods.implicit_q_learning_iql.iql_utils\n",
    "importlib.reload(agent_methods.implicit_q_learning_iql.iql_utils)\n",
    "\n",
    "from agent_methods.implicit_q_learning_iql.iql_utils import train_and_evaluate_IQL\n",
    "\n",
    "import agent_methods.behavior_value_estimation_bve.bve_utils\n",
    "importlib.reload(agent_methods.behavior_value_estimation_bve.bve_utils)\n",
    "\n",
    "from agent_methods.behavior_value_estimation_bve.bve_utils import train_and_evaluate_BVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 12345\n",
    "ENV_ID = 'SeaquestNoFrameskip-v4'\n",
    "EPOCHS = 10\n",
    "SEEDS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# set seed for reproducability\n",
    "set_all_seeds(SEED)\n",
    "\n",
    "# force PyTorch to use CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# initialize enviornment\n",
    "env = create_environment(env_id=ENV_ID, seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training all agents on: Beginner Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0% Perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading seaquest_beginner_perturb0 dataset...\n",
      "Data preprocessing for seaquest_beginner_perturb0 dataset...\n",
      "Creating dataloaders for seaquest_beginner_perturb0 dataset...\n",
      "dict_keys(['seaquest_beginner_perturb0'])\n"
     ]
    }
   ],
   "source": [
    "# Paths to your datasets\n",
    "dataset_path = 'datasets/beginner_logs/seaquest_beginner_perturb0.pkl'\n",
    "\n",
    "dataloaders = {}  # Store dataloaders for each dataset\n",
    "\n",
    "dataset_name = os.path.splitext(os.path.basename(dataset_path))[0]\n",
    "\n",
    "print(f\"Loading {dataset_name} dataset...\")\n",
    "\n",
    "# Load dataset\n",
    "data = load_dataset(dataset_path)\n",
    "\n",
    "print(f\"Data preprocessing for {dataset_name} dataset...\")\n",
    "\n",
    "# Preprocess and split the data\n",
    "train_data, test_data, tune_data = preprocess_and_split(\n",
    "    data=data, seed=SEED, test_size=0.2, tune_size=0.1\n",
    ")\n",
    "\n",
    "print(f\"Creating dataloaders for {dataset_name} dataset...\")\n",
    "\n",
    "# Create dataloaders using the adjusted function\n",
    "train_loader, test_loader, tune_loader = create_dataloaders(\n",
    "    train_data, test_data, tune_data, batch_size=64, seed=SEED\n",
    ")\n",
    "\n",
    "# Store dataloaders\n",
    "dataloaders[dataset_name] = {\n",
    "    'train': train_loader,\n",
    "    'test': test_loader,\n",
    "    'tuning': tune_loader\n",
    "}\n",
    "\n",
    "# Clear variables to free up memory\n",
    "del data, train_data, test_data, tune_data\n",
    "gc.collect()\n",
    "\n",
    "print(dataloaders.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BC on seaquest_beginner_perturb0\n",
      "-- Starting Seed 1/3 --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 10/10 [57:38<00:00, 345.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training on seaquest_beginner_perturb0\n",
      "    ➤ Avg Train Loss: -1.68118\n",
      "    ➤ Avg Test Loss: -1.71870\n",
      "    ➤ Avg Reward: 262.00\n",
      "Model saved to agent_methods/behavioral_cloning_bc/bc_logs/seaquest_beginner/perturb0/bc_model_perturb0.pth\n",
      "-- Starting Seed 2/3 --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 10/10 [57:16<00:00, 343.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training on seaquest_beginner_perturb0\n",
      "    ➤ Avg Train Loss: -1.67643\n",
      "    ➤ Avg Test Loss: -1.89362\n",
      "    ➤ Avg Reward: 248.00\n",
      "-- Starting Seed 3/3 --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 10/10 [57:05<00:00, 342.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training on seaquest_beginner_perturb0\n",
      "    ➤ Avg Train Loss: -1.69835\n",
      "    ➤ Avg Test Loss: -1.67948\n",
      "    ➤ Avg Reward: 260.00\n",
      "Return Stats saved to agent_methods/behavioral_cloning_bc/bc_logs/seaquest_beginner/perturb0/stats_perturb0.pkl\n",
      "----- Execution time: BC - Beginner | Perturbation 0% -----\n",
      "CPU times: total: 3h 31min 37s\n",
      "Wall time: 2h 52min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# train and evaluate the BC model on the Expert dataset with 0% perturbation\n",
    "train_and_evaluate_BC(\n",
    "    dataloaders=dataloaders,\n",
    "    device=device,\n",
    "    seeds=SEEDS,\n",
    "    epochs=EPOCHS,\n",
    "    dataset='seaquest_beginner_perturb0',\n",
    "    env_id=ENV_ID,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# print execution time it took to train the model\n",
    "print(\"----- Execution time: BC - Beginner | Perturbation 0% -----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training IQL on seaquest_beginner_perturb0\n",
      "-- Starting Seed 1/3 --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# train and evaluate the IQL model on the Beginner dataset with 0% perturbation\n",
    "train_and_evaluate_IQL(\n",
    "    dataloaders=dataloaders,\n",
    "    device=device,\n",
    "    seeds=SEEDS,\n",
    "    epochs=EPOCHS,\n",
    "    dataset='seaquest_beginner_perturb0',\n",
    "    env_id=ENV_ID,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# print execution time it took to train the model\n",
    "print(\"----- Execution time: IQL - Beginner | Perturbation 0% -----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BVE on seaquest_beginner_perturb0\n",
      "-- Starting Seed 1/3 --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 10/10 [1:06:34<00:00, 399.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training on seaquest_beginner_perturb0\n",
      "    ➤ Avg Train Loss: -1.56911\n",
      "    ➤ Avg Test Loss: -1.46587\n",
      "    ➤ Avg Reward: 20.00\n",
      "Saved model to agent_methods/behavior_value_estimation/bve_logs/seaquest_beginner/perturb0/bve_model_perturb0.pth\n",
      "-- Starting Seed 2/3 --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 10/10 [1:07:12<00:00, 403.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training on seaquest_beginner_perturb0\n",
      "    ➤ Avg Train Loss: -1.61088\n",
      "    ➤ Avg Test Loss: -1.50807\n",
      "    ➤ Avg Reward: 0.00\n",
      "-- Starting Seed 3/3 --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 10/10 [1:06:48<00:00, 400.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training on seaquest_beginner_perturb0\n",
      "    ➤ Avg Train Loss: -1.56141\n",
      "    ➤ Avg Test Loss: -1.48893\n",
      "    ➤ Avg Reward: 44.00\n",
      "Saved stats to agent_methods/behavior_value_estimation/bve_logs/seaquest_beginner/perturb0/stats_perturb0.pkl\n",
      "----- Execution time: BVE - Beginner | Perturbation 0% -----\n",
      "CPU times: total: 3h 56min 33s\n",
      "Wall time: 3h 20min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# train and evaluate the BVE model on the Beginner dataset with 0% perturbation\n",
    "train_and_evaluate_BVE(\n",
    "    dataloaders=dataloaders,\n",
    "    device=device,\n",
    "    seeds=SEEDS,\n",
    "    epochs=EPOCHS,\n",
    "    dataset='seaquest_beginner_perturb0',\n",
    "    env_id=ENV_ID,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# print execution time it took to train the model\n",
    "print(\"----- Execution time: BVE - Beginner | Perturbation 0% -----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5% Perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading expert_dataset_perturbation_5 dataset...\n",
      "Data preprocessing for expert_dataset_perturbation_5 dataset...\n",
      "Creating dataloaders for expert_dataset_perturbation_5 dataset...\n",
      "dict_keys(['expert_dataset_perturbation_5'])\n"
     ]
    }
   ],
   "source": [
    "# Paths to your datasets\n",
    "dataset_paths = [\n",
    "    'datasets/expert/expert_logs/expert_dataset_perturbation_5.pkl',\n",
    "]\n",
    "\n",
    "dataloaders = {}  # Store dataloaders for each dataset\n",
    "\n",
    "for path in dataset_paths:\n",
    "    dataset_name = path.split('/')[-1].split('.')[0]  # Extract dataset name\n",
    "\n",
    "    print(f\"Loading {dataset_name} dataset...\")\n",
    "\n",
    "    # Load dataset\n",
    "    data = load_dataset(path)\n",
    "\n",
    "    print(f\"Data preprocessing for {dataset_name} dataset...\")\n",
    "\n",
    "    # Preprocess and split the data\n",
    "    train_data, test_data, tune_data = preprocess_and_split(\n",
    "        data=data, seed=SEED, test_size=0.2, tune_size=0.1\n",
    "    )\n",
    "\n",
    "    print(f\"Creating dataloaders for {dataset_name} dataset...\")\n",
    "\n",
    "    # Create dataloaders using the adjusted function\n",
    "    train_loader, test_loader, tune_loader = create_dataloaders(\n",
    "        train_data, test_data, tune_data, batch_size=64, seed=SEED\n",
    "    )\n",
    "\n",
    "    # Store dataloaders\n",
    "    dataloaders[dataset_name] = {\n",
    "        'train': train_loader,\n",
    "        'test': test_loader,\n",
    "        'tuning': tune_loader\n",
    "    }\n",
    "\n",
    "    # Clear variables to free up memory\n",
    "    del data, train_data, test_data, tune_data\n",
    "    gc.collect()\n",
    "\n",
    "print(dataloaders.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting expert_dataset_perturbation_5 Training Set:\n",
      "--- Sample 1 ---\n",
      "States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Actions Batch Shape: torch.Size([64])\n",
      "Rewards Batch Shape: torch.Size([64])\n",
      "Next States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Dones Batch Shape: torch.Size([64])\n",
      "States Batch Data Type: torch.float32\n",
      "Actions Batch Data Type: torch.int64\n",
      "Rewards Batch Data Type: torch.float32\n",
      "Next States Batch Data Type: torch.float32\n",
      "Dones Batch Data Type: torch.float32\n",
      "\n",
      "First Sample Details:\n",
      "First State Shape: torch.Size([3, 210, 160])\n",
      "First State Min/Max: 0.0000/0.8392\n",
      "First Action: 8\n",
      "First Reward: 0.0000\n",
      "First Next State Shape: torch.Size([3, 210, 160])\n",
      "First Done: 0.0\n",
      "\n",
      "\n",
      "Inspecting expert_dataset_perturbation_5 Testing Set\n",
      "--- Sample 1 ---\n",
      "States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Actions Batch Shape: torch.Size([64])\n",
      "Rewards Batch Shape: torch.Size([64])\n",
      "Next States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Dones Batch Shape: torch.Size([64])\n",
      "States Batch Data Type: torch.float32\n",
      "Actions Batch Data Type: torch.int64\n",
      "Rewards Batch Data Type: torch.float32\n",
      "Next States Batch Data Type: torch.float32\n",
      "Dones Batch Data Type: torch.float32\n",
      "\n",
      "First Sample Details:\n",
      "First State Shape: torch.Size([3, 210, 160])\n",
      "First State Min/Max: 0.0000/0.8392\n",
      "First Action: 4\n",
      "First Reward: 0.0000\n",
      "First Next State Shape: torch.Size([3, 210, 160])\n",
      "First Done: 0.0\n",
      "\n",
      "\n",
      "Inspecting expert_dataset_perturbation_5 Tuning Set\n",
      "--- Sample 1 ---\n",
      "States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Actions Batch Shape: torch.Size([64])\n",
      "Rewards Batch Shape: torch.Size([64])\n",
      "Next States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Dones Batch Shape: torch.Size([64])\n",
      "States Batch Data Type: torch.float32\n",
      "Actions Batch Data Type: torch.int64\n",
      "Rewards Batch Data Type: torch.float32\n",
      "Next States Batch Data Type: torch.float32\n",
      "Dones Batch Data Type: torch.float32\n",
      "\n",
      "First Sample Details:\n",
      "First State Shape: torch.Size([3, 210, 160])\n",
      "First State Min/Max: 0.0000/0.8392\n",
      "First Action: 8\n",
      "First Reward: 0.0000\n",
      "First Next State Shape: torch.Size([3, 210, 160])\n",
      "First Done: 0.0\n"
     ]
    }
   ],
   "source": [
    "# inspect dataset samples\n",
    "dataset_name_to_inspect = 'expert_dataset_perturbation_5'\n",
    "\n",
    "# inspect the training set of the dataset\n",
    "print(f\"Inspecting {dataset_name_to_inspect} Training Set:\")\n",
    "inspect_dataset_sample(dataloaders[dataset_name_to_inspect]['train'])\n",
    "print(\"\\n\")\n",
    "\n",
    "# inspect the testing set of the dataset\n",
    "print(f\"Inspecting {dataset_name_to_inspect} Testing Set\")\n",
    "inspect_dataset_sample(dataloaders[dataset_name_to_inspect]['test'])\n",
    "print(\"\\n\")\n",
    "\n",
    "# inspect the tuning set of the dataset\n",
    "print(f\"Inspecting {dataset_name_to_inspect} Tuning Set\")\n",
    "inspect_dataset_sample(dataloaders[dataset_name_to_inspect]['tuning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on expert_dataset_perturbation_5\n",
      "-- Starting Trial 1/1 --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 10/10 [1:07:02<00:00, 402.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial learning rate: 0.001000\n",
      "Adjusted learning rate in epoch 10: 0.000900\n",
      "Finished Training on expert_dataset_perturbation_5 - Training Loss: 0.25136\n",
      "                                                   - Tuning Loss: 1.60741\n",
      "                                                   - Test Loss: 1.55508\n",
      "                                                   - Reward: 260.00\n",
      "Model saved to behavioral_cloning_bc/bc_logs/expert_dataset/perturbation_5/bc_model_5.pth\n",
      "Return Stats saved to behavioral_cloning_bc/bc_logs/expert_dataset/perturbation_5/stats_5.pkl\n",
      "----- Execution time: BC - Expert | Perturbation 5% -----\n",
      "CPU times: total: 1h 20min 30s\n",
      "Wall time: 1h 7min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# train and evaluate the BC model on the Expert dataset with 5% perturbation\n",
    "train_and_evaluate_BC(\n",
    "    dataloaders=dataloaders,\n",
    "    device=device,\n",
    "    trials=1,\n",
    "    epochs=EPOCHS,\n",
    "    dataset='expert_dataset_perturbation_5',\n",
    "    env_id=ENV_ID,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# print execution time it took to train the model\n",
    "print(\"----- Execution time: BC - Expert | Perturbation 5% -----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### continue training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuing BC Training on expert_dataset_perturbation_5\n",
      "Loading existing model from behavioral_cloning_bc/bc_logs/expert_dataset/perturbation_5/bc_model_5.pth\n",
      "Loading existing stats from behavioral_cloning_bc/bc_logs/expert_dataset/perturbation_5/stats_5.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Continued Training Epochs: 100%|██████████| 10/10 [1:11:00<00:00, 426.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished additional Training on expert_dataset_perturbation_5 - Training Loss: 0.11232\n",
      "                                                              - Tuning Loss: 2.42555\n",
      "                                                              - Test Loss: 2.36914\n",
      "                                                              - Reward: 600.00\n",
      "Updated DQN model saved to behavioral_cloning_bc/bc_logs/expert_dataset/perturbation_5/bc_model_5_continued.pth\n",
      "Updated stats saved to behavioral_cloning_bc/bc_logs/expert_dataset/perturbation_5/stats_5_continued.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# continue training for 10 more epochs\n",
    "continue_training_BC(\n",
    "    dataloaders=dataloaders,\n",
    "    further_epochs=10,\n",
    "    dataset=\"expert_dataset_perturbation_5\",\n",
    "    env_id=ENV_ID,\n",
    "    seed=SEED,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training IQL on expert_dataset_perturbation_5\n",
      "-- Starting Trial 1/1 --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 10/10 [1:13:05<00:00, 438.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training on expert_dataset_perturbation_5 - Actor Loss: 5.36442\n",
      "                                                   - Critic 1 Loss: 0.22395\n",
      "                                                   - Critic 2 Loss: 0.17995\n",
      "                                                   - Value Loss: 0.34945\n",
      "                                                   - Tuning Loss: 1.06684\n",
      "                                                   - Test Loss: 1.05855\n",
      "                                                   - Reward: 344.00000\n",
      "Model saved to implicit_q_learning_iql/iql_logs/expert_dataset/perturbation_5/iql_model_5.pth\n",
      "Return Stats saved to implicit_q_learning_iql/iql_logs/expert_dataset/perturbation_5/stats_5.pkl\n",
      "----- Execution time: IQL - Expert | Perturbation 5% -----\n",
      "CPU times: total: 1h 27min 16s\n",
      "Wall time: 1h 13min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# train and evaluate the IQL model on the Expert dataset with 5% perturbation\n",
    "train_and_evaluate_IQL(\n",
    "    dataloaders=dataloaders,\n",
    "    device=device,\n",
    "    trials=1,\n",
    "    epochs=EPOCHS,\n",
    "    dataset='expert_dataset_perturbation_5',\n",
    "    env_id=ENV_ID,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# print execution time it took to train the model\n",
    "print(\"----- Execution time: IQL - Expert | Perturbation 5% -----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### continue training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuing IQL Training on expert_dataset_perturbation_5\n",
      "Loading existing model from implicit_q_learning_iql/iql_logs/expert_dataset/perturbation_5/iql_model_5.pth\n",
      "Loading existing stats from implicit_q_learning_iql/iql_logs/expert_dataset/perturbation_5/stats_5.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Continued Training Epochs: 100%|██████████| 10/10 [1:18:49<00:00, 472.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished additional Training on expert_dataset_perturbation_5 - Actor Loss: 2.89492\n",
      "                                                              - Critic 1 Loss: 0.18770\n",
      "                                                              - Critic 2 Loss: 0.18581\n",
      "                                                              - Value Loss: 0.01366\n",
      "                                                              - Tuning Loss: 1.25915\n",
      "                                                              - Test Loss: 1.22187\n",
      "                                                              - Reward: 319.00\n",
      "Updated DQN model saved to implicit_q_learning_iql/iql_logs/expert_dataset/perturbation_5/iql_model_5_continued.pth\n",
      "Updated stats saved to implicit_q_learning_iql/iql_logs/expert_dataset/perturbation_5/stats_5_continued.pkl\n"
     ]
    }
   ],
   "source": [
    "# continue training for 10 more epochs\n",
    "continue_training_IQL(\n",
    "    dataloaders=dataloaders,\n",
    "    further_epochs=10,\n",
    "    dataset=\"expert_dataset_perturbation_5\",\n",
    "    env_id=ENV_ID,\n",
    "    seed=SEED,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on expert_dataset_perturbation_5\n",
      "Filling replay buffer from dataset...\n",
      "Replay buffer filled with 108439 samples.\n",
      "-- Starting Trial 1/1 --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 10/10 [54:23<00:00, 326.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial learning rate: 0.000500\n",
      "Adjusted learning rate in epoch 8: 0.000450\n",
      "Finished Training on expert_dataset_perturbation_5 - Training Loss: 0.13587\n",
      "                                                   - Tuning Loss: 0.15067\n",
      "                                                   - Test Loss: 0.14560\n",
      "                                                   - Reward: 0.00\n",
      "Model saved to deep_q_network_dqn/dqn_logs/expert_dataset/perturbation_5/dqn_model_5.pth\n",
      "Return Stats saved to deep_q_network_dqn/dqn_logs/expert_dataset/perturbation_5/stats_5.pkl\n",
      "----- Execution time: DQN - Expert | Perturbation 5% -----\n",
      "CPU times: total: 1h 5min 31s\n",
      "Wall time: 56min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# train DQN agent on Expert dataset with 5% perturbation\n",
    "train_and_evaluate_DQN(\n",
    "    dataloaders=dataloaders,\n",
    "    device=device,\n",
    "    trials=1,\n",
    "    epochs=EPOCHS, \n",
    "    dataset='expert_dataset_perturbation_5',\n",
    "    env_id=ENV_ID,\n",
    "    seed=SEED \n",
    ")\n",
    "\n",
    "# print execution time it took to train the model\n",
    "print(\"----- Execution time: DQN - Expert | Perturbation 5% -----\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### continue training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuing DQN Training on expert_dataset_perturbation_5\n",
      "Loading existing model from deep_q_network_dqn/dqn_logs/expert_dataset/perturbation_5/dqn_model_5.pth\n",
      "Loading existing stats from deep_q_network_dqn/dqn_logs/expert_dataset/perturbation_5/stats_5.pkl\n",
      "Filling replay buffer...\n",
      "Replay buffer filled with 108439 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Continued DQN Epochs: 100%|██████████| 10/10 [55:54<00:00, 335.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished additional Training on expert_dataset_perturbation_5 - Training Loss: 0.14865\n",
      "                                                              - Tuning Loss: 0.15051\n",
      "                                                              - Test Loss: 0.14544\n",
      "                                                              - Reward: 0.00\n",
      "Updated DQN model saved to deep_q_network_dqn/dqn_logs/expert_dataset/perturbation_5/dqn_model_5_continued.pth\n",
      "Updated stats saved to deep_q_network_dqn/dqn_logs/expert_dataset/perturbation_5/stats_5_continued.pkl\n"
     ]
    }
   ],
   "source": [
    "# Continue training for 10 more epochs\n",
    "continue_training_DQN(\n",
    "    dataloaders=dataloaders,\n",
    "    further_epochs=10,\n",
    "    dataset=\"expert_dataset_perturbation_5\",\n",
    "    env_id=ENV_ID,\n",
    "    seed=SEED,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10% Perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading expert_dataset_perturbation_10 dataset...\n",
      "Data preprocessing for expert_dataset_perturbation_10 dataset...\n",
      "Creating dataloaders for expert_dataset_perturbation_10 dataset...\n",
      "dict_keys(['expert_dataset_perturbation_10'])\n"
     ]
    }
   ],
   "source": [
    "# Paths to your datasets\n",
    "dataset_paths = [\n",
    "    'datasets/expert/expert_logs/expert_dataset_perturbation_10.pkl',\n",
    "]\n",
    "\n",
    "dataloaders = {}  # Store dataloaders for each dataset\n",
    "\n",
    "for path in dataset_paths:\n",
    "    dataset_name = path.split('/')[-1].split('.')[0]  # Extract dataset name\n",
    "\n",
    "    print(f\"Loading {dataset_name} dataset...\")\n",
    "\n",
    "    # Load dataset\n",
    "    data = load_dataset(path)\n",
    "\n",
    "    print(f\"Data preprocessing for {dataset_name} dataset...\")\n",
    "\n",
    "    # Preprocess and split the data\n",
    "    train_data, test_data, tune_data = preprocess_and_split(\n",
    "        data=data, seed=SEED, test_size=0.2, tune_size=0.1\n",
    "    )\n",
    "\n",
    "    print(f\"Creating dataloaders for {dataset_name} dataset...\")\n",
    "\n",
    "    # Create dataloaders using the adjusted function\n",
    "    train_loader, test_loader, tune_loader = create_dataloaders(\n",
    "        train_data, test_data, tune_data, batch_size=64, seed=SEED\n",
    "    )\n",
    "\n",
    "    # Store dataloaders\n",
    "    dataloaders[dataset_name] = {\n",
    "        'train': train_loader,\n",
    "        'test': test_loader,\n",
    "        'tuning': tune_loader\n",
    "    }\n",
    "\n",
    "    # Clear variables to free up memory\n",
    "    del data, train_data, test_data, tune_data\n",
    "    gc.collect()\n",
    "\n",
    "print(dataloaders.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting expert_dataset_perturbation_10 Training Set:\n",
      "--- Sample 1 ---\n",
      "States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Actions Batch Shape: torch.Size([64])\n",
      "Rewards Batch Shape: torch.Size([64])\n",
      "Next States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Dones Batch Shape: torch.Size([64])\n",
      "States Batch Data Type: torch.float32\n",
      "Actions Batch Data Type: torch.int64\n",
      "Rewards Batch Data Type: torch.float32\n",
      "Next States Batch Data Type: torch.float32\n",
      "Dones Batch Data Type: torch.float32\n",
      "\n",
      "First Sample Details:\n",
      "First State Shape: torch.Size([3, 210, 160])\n",
      "First State Min/Max: 0.0000/0.8392\n",
      "First Action: 9\n",
      "First Reward: 0.0000\n",
      "First Next State Shape: torch.Size([3, 210, 160])\n",
      "First Done: 0.0\n",
      "\n",
      "\n",
      "Inspecting expert_dataset_perturbation_10 Testing Set\n",
      "--- Sample 1 ---\n",
      "States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Actions Batch Shape: torch.Size([64])\n",
      "Rewards Batch Shape: torch.Size([64])\n",
      "Next States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Dones Batch Shape: torch.Size([64])\n",
      "States Batch Data Type: torch.float32\n",
      "Actions Batch Data Type: torch.int64\n",
      "Rewards Batch Data Type: torch.float32\n",
      "Next States Batch Data Type: torch.float32\n",
      "Dones Batch Data Type: torch.float32\n",
      "\n",
      "First Sample Details:\n",
      "First State Shape: torch.Size([3, 210, 160])\n",
      "First State Min/Max: 0.0000/0.8392\n",
      "First Action: 17\n",
      "First Reward: 0.0000\n",
      "First Next State Shape: torch.Size([3, 210, 160])\n",
      "First Done: 0.0\n",
      "\n",
      "\n",
      "Inspecting expert_dataset_perturbation_10 Tuning Set\n",
      "--- Sample 1 ---\n",
      "States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Actions Batch Shape: torch.Size([64])\n",
      "Rewards Batch Shape: torch.Size([64])\n",
      "Next States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Dones Batch Shape: torch.Size([64])\n",
      "States Batch Data Type: torch.float32\n",
      "Actions Batch Data Type: torch.int64\n",
      "Rewards Batch Data Type: torch.float32\n",
      "Next States Batch Data Type: torch.float32\n",
      "Dones Batch Data Type: torch.float32\n",
      "\n",
      "First Sample Details:\n",
      "First State Shape: torch.Size([3, 210, 160])\n",
      "First State Min/Max: 0.0000/0.8392\n",
      "First Action: 13\n",
      "First Reward: 0.0000\n",
      "First Next State Shape: torch.Size([3, 210, 160])\n",
      "First Done: 0.0\n"
     ]
    }
   ],
   "source": [
    "# inspect dataset samples\n",
    "dataset_name_to_inspect = 'expert_dataset_perturbation_10'\n",
    "\n",
    "# inspect the training set of the dataset\n",
    "print(f\"Inspecting {dataset_name_to_inspect} Training Set:\")\n",
    "inspect_dataset_sample(dataloaders[dataset_name_to_inspect]['train'])\n",
    "print(\"\\n\")\n",
    "\n",
    "# inspect the testing set of the dataset\n",
    "print(f\"Inspecting {dataset_name_to_inspect} Testing Set\")\n",
    "inspect_dataset_sample(dataloaders[dataset_name_to_inspect]['test'])\n",
    "print(\"\\n\")\n",
    "\n",
    "# inspect the tuning set of the dataset\n",
    "print(f\"Inspecting {dataset_name_to_inspect} Tuning Set\")\n",
    "inspect_dataset_sample(dataloaders[dataset_name_to_inspect]['tuning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on expert_dataset_perturbation_10\n",
      "-- Starting Trial 1/1 --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 10/10 [1:06:48<00:00, 400.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial learning rate: 0.001000\n",
      "Adjusted learning rate in epoch 10: 0.000900\n",
      "Finished Training on expert_dataset_perturbation_10 - Training Loss: 0.34025\n",
      "                                                   - Tuning Loss: 2.17573\n",
      "                                                   - Test Loss: 2.12246\n",
      "                                                   - Reward: 400.00\n",
      "Model saved to behavioral_cloning_bc/bc_logs/expert_dataset/perturbation_10/bc_model_10.pth\n",
      "Return Stats saved to behavioral_cloning_bc/bc_logs/expert_dataset/perturbation_10/stats_10.pkl\n",
      "----- Execution time: BC - Expert | Perturbation 10% -----\n",
      "CPU times: total: 1h 19min 56s\n",
      "Wall time: 1h 6min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# train and evaluate the BC model on the Expert dataset with 10% perturbation\n",
    "train_and_evaluate_BC(\n",
    "    dataloaders=dataloaders,\n",
    "    device=device,\n",
    "    trials=1,\n",
    "    epochs=EPOCHS,\n",
    "    dataset='expert_dataset_perturbation_10',\n",
    "    env_id=ENV_ID,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# print execution time it took to train the model\n",
    "print(\"----- Execution time: BC - Expert | Perturbation 10% -----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### continue training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuing BC Training on expert_dataset_perturbation_10\n",
      "Loading existing model from behavioral_cloning_bc/bc_logs/expert_dataset/perturbation_10/bc_model_10.pth\n",
      "Loading existing stats from behavioral_cloning_bc/bc_logs/expert_dataset/perturbation_10/stats_10.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Continued Training Epochs: 100%|██████████| 10/10 [1:11:24<00:00, 428.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished additional Training on expert_dataset_perturbation_10 - Training Loss: 0.17036\n",
      "                                                              - Tuning Loss: 3.00531\n",
      "                                                              - Test Loss: 2.93343\n",
      "                                                              - Reward: 240.00\n",
      "Updated DQN model saved to behavioral_cloning_bc/bc_logs/expert_dataset/perturbation_10/bc_model_10_continued.pth\n",
      "Updated stats saved to behavioral_cloning_bc/bc_logs/expert_dataset/perturbation_10/stats_10_continued.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# continue training for 10 more epochs\n",
    "continue_training_BC(\n",
    "    dataloaders=dataloaders,\n",
    "    further_epochs=10,\n",
    "    dataset=\"expert_dataset_perturbation_10\",\n",
    "    env_id=ENV_ID,\n",
    "    seed=SEED,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training IQL on expert_dataset_perturbation_10\n",
      "-- Starting Trial 1/1 --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 10/10 [1:15:46<00:00, 454.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training on expert_dataset_perturbation_10 - Actor Loss: 7.07386\n",
      "                                                   - Critic 1 Loss: 0.50477\n",
      "                                                   - Critic 2 Loss: 0.63776\n",
      "                                                   - Value Loss: 1.58096\n",
      "                                                   - Tuning Loss: 1.31335\n",
      "                                                   - Test Loss: 1.30949\n",
      "                                                   - Reward: 358.00000\n",
      "Model saved to implicit_q_learning_iql/iql_logs/expert_dataset/perturbation_10/iql_model_10.pth\n",
      "Return Stats saved to implicit_q_learning_iql/iql_logs/expert_dataset/perturbation_10/stats_10.pkl\n",
      "----- Execution time: IQL - Expert | Perturbation 10% -----\n",
      "CPU times: total: 1h 28min 34s\n",
      "Wall time: 1h 15min 47s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# train and evaluate the IQL model on the Expert dataset with 10% perturbation\n",
    "train_and_evaluate_IQL(\n",
    "    dataloaders=dataloaders,\n",
    "    device=device,\n",
    "    trials=1,\n",
    "    epochs=EPOCHS,\n",
    "    dataset='expert_dataset_perturbation_10',\n",
    "    env_id=ENV_ID,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# print execution time it took to train the model\n",
    "print(\"----- Execution time: IQL - Expert | Perturbation 10% -----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### continue training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuing IQL Training on expert_dataset_perturbation_10\n",
      "Loading existing model from implicit_q_learning_iql/iql_logs/expert_dataset/perturbation_10/iql_model_10.pth\n",
      "Loading existing stats from implicit_q_learning_iql/iql_logs/expert_dataset/perturbation_10/stats_10.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Continued Training Epochs: 100%|██████████| 10/10 [1:17:26<00:00, 464.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished additional Training on expert_dataset_perturbation_10 - Actor Loss: 3.95128\n",
      "                                                              - Critic 1 Loss: 0.41487\n",
      "                                                              - Critic 2 Loss: 0.35210\n",
      "                                                              - Value Loss: 0.19291\n",
      "                                                              - Tuning Loss: 1.54360\n",
      "                                                              - Test Loss: 1.52630\n",
      "                                                              - Reward: 280.00\n",
      "Updated DQN model saved to implicit_q_learning_iql/iql_logs/expert_dataset/perturbation_10/iql_model_10_continued.pth\n",
      "Updated stats saved to implicit_q_learning_iql/iql_logs/expert_dataset/perturbation_10/stats_10_continued.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# continue training for 10 more epochs\n",
    "continue_training_IQL(\n",
    "    dataloaders=dataloaders,\n",
    "    further_epochs=10,\n",
    "    dataset=\"expert_dataset_perturbation_10\",\n",
    "    env_id=ENV_ID,\n",
    "    seed=SEED,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on expert_dataset_perturbation_10\n",
      "Filling replay buffer from dataset...\n",
      "Replay buffer filled with 108220 samples.\n",
      "-- Starting Trial 1/1 --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 10/10 [37:36<00:00, 225.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial learning rate: 0.000500\n",
      "Adjusted learning rate in epoch 10: 0.000450\n",
      "Finished Training on expert_dataset_perturbation_10 - Training Loss: 0.14010\n",
      "                                                   - Tuning Loss: 0.13340\n",
      "                                                   - Test Loss: 0.14226\n",
      "                                                   - Reward: 40.00\n",
      "Model saved to deep_q_network_dqn/dqn_logs/expert_dataset/perturbation_10/dqn_model_10.pth\n",
      "Return Stats saved to deep_q_network_dqn/dqn_logs/expert_dataset/perturbation_10/stats_10.pkl\n",
      "----- Execution time: DQN - Expert | Perturbation 10% -----\n",
      "CPU times: total: 48min 39s\n",
      "Wall time: 39min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# train DQN agent on Expert dataset with 10% perturbation\n",
    "train_and_evaluate_DQN(\n",
    "    dataloaders=dataloaders,\n",
    "    device=device,\n",
    "    trials=1,\n",
    "    epochs=EPOCHS, \n",
    "    dataset='expert_dataset_perturbation_10',\n",
    "    env_id=ENV_ID,\n",
    "    seed=SEED \n",
    ")\n",
    "\n",
    "# print execution time it took to train the model\n",
    "print(\"----- Execution time: DQN - Expert | Perturbation 10% -----\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### continue training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuing DQN Training on expert_dataset_perturbation_10\n",
      "Loading existing model from deep_q_network_dqn/dqn_logs/expert_dataset/perturbation_10/dqn_model_10.pth\n",
      "Loading existing stats from deep_q_network_dqn/dqn_logs/expert_dataset/perturbation_10/stats_10.pkl\n",
      "Filling replay buffer...\n",
      "Replay buffer filled with 108220 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Continued DQN Epochs: 100%|██████████| 10/10 [35:00<00:00, 210.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished additional Training on expert_dataset_perturbation_10 - Training Loss: 0.14137\n",
      "                                                              - Tuning Loss: 0.13286\n",
      "                                                              - Test Loss: 0.14172\n",
      "                                                              - Reward: 40.00\n",
      "Updated DQN model saved to deep_q_network_dqn/dqn_logs/expert_dataset/perturbation_10/dqn_model_10_continued.pth\n",
      "Updated stats saved to deep_q_network_dqn/dqn_logs/expert_dataset/perturbation_10/stats_10_continued.pkl\n"
     ]
    }
   ],
   "source": [
    "# Continue training for 10 more epochs\n",
    "continue_training_DQN(\n",
    "    dataloaders=dataloaders,\n",
    "    further_epochs=10,\n",
    "    dataset=\"expert_dataset_perturbation_10\",\n",
    "    env_id=ENV_ID,\n",
    "    seed=SEED,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20% Perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading expert_dataset_perturbation_20 dataset...\n",
      "Data preprocessing for expert_dataset_perturbation_20 dataset...\n",
      "Creating dataloaders for expert_dataset_perturbation_20 dataset...\n",
      "dict_keys(['expert_dataset_perturbation_20'])\n"
     ]
    }
   ],
   "source": [
    "# Paths to your datasets\n",
    "dataset_paths = [\n",
    "    'datasets/expert/expert_logs/expert_dataset_perturbation_20.pkl',\n",
    "]\n",
    "\n",
    "dataloaders = {}  # Store dataloaders for each dataset\n",
    "\n",
    "for path in dataset_paths:\n",
    "    dataset_name = path.split('/')[-1].split('.')[0]  # Extract dataset name\n",
    "\n",
    "    print(f\"Loading {dataset_name} dataset...\")\n",
    "\n",
    "    # Load dataset\n",
    "    data = load_dataset(path)\n",
    "\n",
    "    print(f\"Data preprocessing for {dataset_name} dataset...\")\n",
    "\n",
    "    # Preprocess and split the data\n",
    "    train_data, test_data, tune_data = preprocess_and_split(\n",
    "        data=data, seed=SEED, test_size=0.2, tune_size=0.1\n",
    "    )\n",
    "\n",
    "    print(f\"Creating dataloaders for {dataset_name} dataset...\")\n",
    "\n",
    "    # Create dataloaders using the adjusted function\n",
    "    train_loader, test_loader, tune_loader = create_dataloaders(\n",
    "        train_data, test_data, tune_data, batch_size=64, seed=SEED\n",
    "    )\n",
    "\n",
    "    # Store dataloaders\n",
    "    dataloaders[dataset_name] = {\n",
    "        'train': train_loader,\n",
    "        'test': test_loader,\n",
    "        'tuning': tune_loader\n",
    "    }\n",
    "\n",
    "    # Clear variables to free up memory\n",
    "    del data, train_data, test_data, tune_data\n",
    "    gc.collect()\n",
    "\n",
    "print(dataloaders.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting expert_dataset_perturbation_20 Training Set:\n",
      "--- Sample 1 ---\n",
      "States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Actions Batch Shape: torch.Size([64])\n",
      "Rewards Batch Shape: torch.Size([64])\n",
      "Next States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Dones Batch Shape: torch.Size([64])\n",
      "States Batch Data Type: torch.float32\n",
      "Actions Batch Data Type: torch.int64\n",
      "Rewards Batch Data Type: torch.float32\n",
      "Next States Batch Data Type: torch.float32\n",
      "Dones Batch Data Type: torch.float32\n",
      "\n",
      "First Sample Details:\n",
      "First State Shape: torch.Size([3, 210, 160])\n",
      "First State Min/Max: 0.0000/0.9255\n",
      "First Action: 15\n",
      "First Reward: 0.0000\n",
      "First Next State Shape: torch.Size([3, 210, 160])\n",
      "First Done: 0.0\n",
      "\n",
      "\n",
      "Inspecting expert_dataset_perturbation_20 Testing Set\n",
      "--- Sample 1 ---\n",
      "States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Actions Batch Shape: torch.Size([64])\n",
      "Rewards Batch Shape: torch.Size([64])\n",
      "Next States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Dones Batch Shape: torch.Size([64])\n",
      "States Batch Data Type: torch.float32\n",
      "Actions Batch Data Type: torch.int64\n",
      "Rewards Batch Data Type: torch.float32\n",
      "Next States Batch Data Type: torch.float32\n",
      "Dones Batch Data Type: torch.float32\n",
      "\n",
      "First Sample Details:\n",
      "First State Shape: torch.Size([3, 210, 160])\n",
      "First State Min/Max: 0.0000/0.8392\n",
      "First Action: 3\n",
      "First Reward: 0.0000\n",
      "First Next State Shape: torch.Size([3, 210, 160])\n",
      "First Done: 0.0\n",
      "\n",
      "\n",
      "Inspecting expert_dataset_perturbation_20 Tuning Set\n",
      "--- Sample 1 ---\n",
      "States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Actions Batch Shape: torch.Size([64])\n",
      "Rewards Batch Shape: torch.Size([64])\n",
      "Next States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Dones Batch Shape: torch.Size([64])\n",
      "States Batch Data Type: torch.float32\n",
      "Actions Batch Data Type: torch.int64\n",
      "Rewards Batch Data Type: torch.float32\n",
      "Next States Batch Data Type: torch.float32\n",
      "Dones Batch Data Type: torch.float32\n",
      "\n",
      "First Sample Details:\n",
      "First State Shape: torch.Size([3, 210, 160])\n",
      "First State Min/Max: 0.0000/0.9255\n",
      "First Action: 4\n",
      "First Reward: 0.0000\n",
      "First Next State Shape: torch.Size([3, 210, 160])\n",
      "First Done: 0.0\n"
     ]
    }
   ],
   "source": [
    "# inspect dataset samples\n",
    "dataset_name_to_inspect = 'expert_dataset_perturbation_20'\n",
    "\n",
    "# inspect the training set of the dataset\n",
    "print(f\"Inspecting {dataset_name_to_inspect} Training Set:\")\n",
    "inspect_dataset_sample(dataloaders[dataset_name_to_inspect]['train'])\n",
    "print(\"\\n\")\n",
    "\n",
    "# inspect the testing set of the dataset\n",
    "print(f\"Inspecting {dataset_name_to_inspect} Testing Set\")\n",
    "inspect_dataset_sample(dataloaders[dataset_name_to_inspect]['test'])\n",
    "print(\"\\n\")\n",
    "\n",
    "# inspect the tuning set of the dataset\n",
    "print(f\"Inspecting {dataset_name_to_inspect} Tuning Set\")\n",
    "inspect_dataset_sample(dataloaders[dataset_name_to_inspect]['tuning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on expert_dataset_perturbation_20\n",
      "-- Starting Trial 1/1 --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 10/10 [1:04:07<00:00, 384.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial learning rate: 0.001000\n",
      "Adjusted learning rate in epoch 9: 0.000900\n",
      "Finished Training on expert_dataset_perturbation_20 - Training Loss: 0.52623\n",
      "                                                   - Tuning Loss: 2.80368\n",
      "                                                   - Test Loss: 2.79225\n",
      "                                                   - Reward: 500.00\n",
      "Model saved to behavioral_cloning_bc/bc_logs/expert_dataset/perturbation_20/bc_model_20.pth\n",
      "Return Stats saved to behavioral_cloning_bc/bc_logs/expert_dataset/perturbation_20/stats_20.pkl\n",
      "----- Execution time: BC - Expert | Perturbation 20% -----\n",
      "CPU times: total: 1h 17min 31s\n",
      "Wall time: 1h 4min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# train and evaluate the BC model on the Expert dataset with 20% perturbation\n",
    "train_and_evaluate_BC(\n",
    "    dataloaders=dataloaders,\n",
    "    device=device,\n",
    "    trials=1,\n",
    "    epochs=EPOCHS,\n",
    "    dataset='expert_dataset_perturbation_20',\n",
    "    env_id=ENV_ID,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# print execution time it took to train the model\n",
    "print(\"----- Execution time: BC - Expert | Perturbation 20% -----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### continue training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuing BC Training on expert_dataset_perturbation_20\n",
      "Loading existing model from behavioral_cloning_bc/bc_logs/expert_dataset/perturbation_20/bc_model_20.pth\n",
      "Loading existing stats from behavioral_cloning_bc/bc_logs/expert_dataset/perturbation_20/stats_20.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Continued Training Epochs: 100%|██████████| 10/10 [1:08:49<00:00, 412.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished additional Training on expert_dataset_perturbation_20 - Training Loss: 0.30311\n",
      "                                                              - Tuning Loss: 3.89714\n",
      "                                                              - Test Loss: 3.85889\n",
      "                                                              - Reward: 340.00\n",
      "Updated DQN model saved to behavioral_cloning_bc/bc_logs/expert_dataset/perturbation_20/bc_model_20_continued.pth\n",
      "Updated stats saved to behavioral_cloning_bc/bc_logs/expert_dataset/perturbation_20/stats_20_continued.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# continue training for 10 more epochs\n",
    "continue_training_BC(\n",
    "    dataloaders=dataloaders,\n",
    "    further_epochs=10,\n",
    "    dataset=\"expert_dataset_perturbation_20\",\n",
    "    env_id=ENV_ID,\n",
    "    seed=SEED,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training IQL on expert_dataset_perturbation_20\n",
      "-- Starting Trial 1/1 --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 10/10 [1:11:04<00:00, 426.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training on expert_dataset_perturbation_20 - Actor Loss: 9.05648\n",
      "                                                   - Critic 1 Loss: 0.25150\n",
      "                                                   - Critic 2 Loss: 0.33957\n",
      "                                                   - Value Loss: 1.78412\n",
      "                                                   - Tuning Loss: 1.69753\n",
      "                                                   - Test Loss: 1.69888\n",
      "                                                   - Reward: 302.00000\n",
      "Model saved to implicit_q_learning_iql/iql_logs/expert_dataset/perturbation_20/iql_model_20.pth\n",
      "Return Stats saved to implicit_q_learning_iql/iql_logs/expert_dataset/perturbation_20/stats_20.pkl\n",
      "----- Execution time: IQL - Expert | Perturbation 20% -----\n",
      "CPU times: total: 1h 23min 20s\n",
      "Wall time: 1h 11min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# train and evaluate the IQL model on the Expert dataset with 20% perturbation\n",
    "train_and_evaluate_IQL(\n",
    "    dataloaders=dataloaders,\n",
    "    device=device,\n",
    "    trials=1,\n",
    "    epochs=EPOCHS,\n",
    "    dataset='expert_dataset_perturbation_20',\n",
    "    env_id=ENV_ID,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# print execution time it took to train the model\n",
    "print(\"----- Execution time: IQL - Expert | Perturbation 20% -----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### continue training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuing IQL Training on expert_dataset_perturbation_20\n",
      "Loading existing model from implicit_q_learning_iql/iql_logs/expert_dataset/perturbation_20/iql_model_20.pth\n",
      "Loading existing stats from implicit_q_learning_iql/iql_logs/expert_dataset/perturbation_20/stats_20.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Continued Training Epochs: 100%|██████████| 10/10 [1:14:06<00:00, 444.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished additional Training on expert_dataset_perturbation_20 - Actor Loss: 5.71745\n",
      "                                                              - Critic 1 Loss: 0.17306\n",
      "                                                              - Critic 2 Loss: 0.17535\n",
      "                                                              - Value Loss: 0.01386\n",
      "                                                              - Tuning Loss: 2.06185\n",
      "                                                              - Test Loss: 2.05134\n",
      "                                                              - Reward: 305.00\n",
      "Updated DQN model saved to implicit_q_learning_iql/iql_logs/expert_dataset/perturbation_20/iql_model_20_continued.pth\n",
      "Updated stats saved to implicit_q_learning_iql/iql_logs/expert_dataset/perturbation_20/stats_20_continued.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# continue training for 10 more epochs\n",
    "continue_training_IQL(\n",
    "    dataloaders=dataloaders,\n",
    "    further_epochs=10,\n",
    "    dataset=\"expert_dataset_perturbation_20\",\n",
    "    env_id=ENV_ID,\n",
    "    seed=SEED,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on expert_dataset_perturbation_20\n",
      "Filling replay buffer from dataset...\n",
      "Replay buffer filled with 105804 samples.\n",
      "-- Starting Trial 1/1 --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 10/10 [52:32<00:00, 315.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial learning rate: 0.000500\n",
      "Adjusted learning rate in epoch 7: 0.000450\n",
      "Finished Training on expert_dataset_perturbation_20 - Training Loss: 0.13098\n",
      "                                                   - Tuning Loss: 0.14870\n",
      "                                                   - Test Loss: 0.12809\n",
      "                                                   - Reward: 0.00\n",
      "Model saved to deep_q_network_dqn/dqn_logs/expert_dataset/perturbation_20/dqn_model_20.pth\n",
      "Return Stats saved to deep_q_network_dqn/dqn_logs/expert_dataset/perturbation_20/stats_20.pkl\n",
      "----- Execution time: DQN - Expert | Perturbation 20% -----\n",
      "CPU times: total: 1h 3min 10s\n",
      "Wall time: 54min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# train DQN agent on Expert dataset with 20% perturbation\n",
    "train_and_evaluate_DQN(\n",
    "    dataloaders=dataloaders,\n",
    "    device=device,\n",
    "    trials=1,\n",
    "    epochs=EPOCHS, \n",
    "    dataset='expert_dataset_perturbation_20',\n",
    "    env_id=ENV_ID,\n",
    "    seed=SEED \n",
    ")\n",
    "\n",
    "# print execution time it took to train the model\n",
    "print(\"----- Execution time: DQN - Expert | Perturbation 20% -----\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### continue training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuing DQN Training on expert_dataset_perturbation_20\n",
      "Loading existing model from deep_q_network_dqn/dqn_logs/expert_dataset/perturbation_20/dqn_model_20.pth\n",
      "Loading existing stats from deep_q_network_dqn/dqn_logs/expert_dataset/perturbation_20/stats_20.pkl\n",
      "Filling replay buffer...\n",
      "Replay buffer filled with 105804 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Continued DQN Epochs: 100%|██████████| 10/10 [51:34<00:00, 309.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished additional Training on expert_dataset_perturbation_20 - Training Loss: 0.13528\n",
      "                                                              - Tuning Loss: 0.14885\n",
      "                                                              - Test Loss: 0.12823\n",
      "                                                              - Reward: 0.00\n",
      "Updated DQN model saved to deep_q_network_dqn/dqn_logs/expert_dataset/perturbation_20/dqn_model_20_continued.pth\n",
      "Updated stats saved to deep_q_network_dqn/dqn_logs/expert_dataset/perturbation_20/stats_20_continued.pkl\n"
     ]
    }
   ],
   "source": [
    "# Continue training for 10 more epochs\n",
    "continue_training_DQN(\n",
    "    dataloaders=dataloaders,\n",
    "    further_epochs=10,\n",
    "    dataset=\"expert_dataset_perturbation_20\",\n",
    "    env_id=ENV_ID,\n",
    "    seed=SEED,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Mixed Agents for BC, IQL and DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0% Perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mixed_dataset_perturbation_0 dataset...\n",
      "Data preprocessing for mixed_dataset_perturbation_0 dataset...\n",
      "Creating dataloaders for mixed_dataset_perturbation_0 dataset...\n",
      "dict_keys(['mixed_dataset_perturbation_0'])\n"
     ]
    }
   ],
   "source": [
    "# Paths to your datasets\n",
    "dataset_paths = [\n",
    "    'datasets/mixed/mixed_logs/mixed_dataset_perturbation_0.pkl',\n",
    "]\n",
    "\n",
    "dataloaders = {}  # Store dataloaders for each dataset\n",
    "\n",
    "for path in dataset_paths:\n",
    "    dataset_name = path.split('/')[-1].split('.')[0]  # Extract dataset name\n",
    "\n",
    "    print(f\"Loading {dataset_name} dataset...\")\n",
    "\n",
    "    # Load dataset\n",
    "    data = load_dataset(path)\n",
    "\n",
    "    print(f\"Data preprocessing for {dataset_name} dataset...\")\n",
    "\n",
    "    # Preprocess and split the data\n",
    "    train_data, test_data, tune_data = preprocess_and_split(\n",
    "        data=data, seed=SEED, test_size=0.2, tune_size=0.1\n",
    "    )\n",
    "\n",
    "    print(f\"Creating dataloaders for {dataset_name} dataset...\")\n",
    "\n",
    "    # Create dataloaders using the adjusted function\n",
    "    train_loader, test_loader, tune_loader = create_dataloaders(\n",
    "        train_data, test_data, tune_data, batch_size=64, seed=SEED\n",
    "    )\n",
    "\n",
    "    # Store dataloaders\n",
    "    dataloaders[dataset_name] = {\n",
    "        'train': train_loader,\n",
    "        'test': test_loader,\n",
    "        'tuning': tune_loader\n",
    "    }\n",
    "\n",
    "    # Clear variables to free up memory\n",
    "    del data, train_data, test_data, tune_data\n",
    "    gc.collect()\n",
    "\n",
    "print(dataloaders.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting mixed_dataset_perturbation_0 Training Set:\n",
      "--- Sample 1 ---\n",
      "States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Actions Batch Shape: torch.Size([64])\n",
      "Rewards Batch Shape: torch.Size([64])\n",
      "Next States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Dones Batch Shape: torch.Size([64])\n",
      "States Batch Data Type: torch.float32\n",
      "Actions Batch Data Type: torch.int64\n",
      "Rewards Batch Data Type: torch.float32\n",
      "Next States Batch Data Type: torch.float32\n",
      "Dones Batch Data Type: torch.float32\n",
      "\n",
      "First Sample Details:\n",
      "First State Shape: torch.Size([3, 210, 160])\n",
      "First State Min/Max: 0.0000/0.8392\n",
      "First Action: 13\n",
      "First Reward: 0.0000\n",
      "First Next State Shape: torch.Size([3, 210, 160])\n",
      "First Done: 0.0\n",
      "\n",
      "\n",
      "Inspecting mixed_dataset_perturbation_0 Testing Set\n",
      "--- Sample 1 ---\n",
      "States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Actions Batch Shape: torch.Size([64])\n",
      "Rewards Batch Shape: torch.Size([64])\n",
      "Next States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Dones Batch Shape: torch.Size([64])\n",
      "States Batch Data Type: torch.float32\n",
      "Actions Batch Data Type: torch.int64\n",
      "Rewards Batch Data Type: torch.float32\n",
      "Next States Batch Data Type: torch.float32\n",
      "Dones Batch Data Type: torch.float32\n",
      "\n",
      "First Sample Details:\n",
      "First State Shape: torch.Size([3, 210, 160])\n",
      "First State Min/Max: 0.0000/0.8392\n",
      "First Action: 8\n",
      "First Reward: 0.0000\n",
      "First Next State Shape: torch.Size([3, 210, 160])\n",
      "First Done: 0.0\n",
      "\n",
      "\n",
      "Inspecting mixed_dataset_perturbation_0 Tuning Set\n",
      "--- Sample 1 ---\n",
      "States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Actions Batch Shape: torch.Size([64])\n",
      "Rewards Batch Shape: torch.Size([64])\n",
      "Next States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Dones Batch Shape: torch.Size([64])\n",
      "States Batch Data Type: torch.float32\n",
      "Actions Batch Data Type: torch.int64\n",
      "Rewards Batch Data Type: torch.float32\n",
      "Next States Batch Data Type: torch.float32\n",
      "Dones Batch Data Type: torch.float32\n",
      "\n",
      "First Sample Details:\n",
      "First State Shape: torch.Size([3, 210, 160])\n",
      "First State Min/Max: 0.0000/0.8392\n",
      "First Action: 13\n",
      "First Reward: 0.0000\n",
      "First Next State Shape: torch.Size([3, 210, 160])\n",
      "First Done: 0.0\n"
     ]
    }
   ],
   "source": [
    "# inspect dataset samples\n",
    "dataset_name_to_inspect = 'mixed_dataset_perturbation_0'\n",
    "\n",
    "# inspect the training set of the dataset\n",
    "print(f\"Inspecting {dataset_name_to_inspect} Training Set:\")\n",
    "inspect_dataset_sample(dataloaders[dataset_name_to_inspect]['train'])\n",
    "print(\"\\n\")\n",
    "\n",
    "# inspect the testing set of the dataset\n",
    "print(f\"Inspecting {dataset_name_to_inspect} Testing Set\")\n",
    "inspect_dataset_sample(dataloaders[dataset_name_to_inspect]['test'])\n",
    "print(\"\\n\")\n",
    "\n",
    "# inspect the tuning set of the dataset\n",
    "print(f\"Inspecting {dataset_name_to_inspect} Tuning Set\")\n",
    "inspect_dataset_sample(dataloaders[dataset_name_to_inspect]['tuning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### continue training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### continue training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### continue training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5% Perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mixed_dataset_perturbation_5 dataset...\n",
      "Data preprocessing for mixed_dataset_perturbation_5 dataset...\n",
      "Creating dataloaders for mixed_dataset_perturbation_5 dataset...\n",
      "dict_keys(['mixed_dataset_perturbation_5'])\n"
     ]
    }
   ],
   "source": [
    "# Paths to your datasets\n",
    "dataset_paths = [\n",
    "    'datasets/mixed/mixed_logs/mixed_dataset_perturbation_5.pkl',\n",
    "]\n",
    "\n",
    "dataloaders = {}  # Store dataloaders for each dataset\n",
    "\n",
    "for path in dataset_paths:\n",
    "    dataset_name = path.split('/')[-1].split('.')[0]  # Extract dataset name\n",
    "\n",
    "    print(f\"Loading {dataset_name} dataset...\")\n",
    "\n",
    "    # Load dataset\n",
    "    data = load_dataset(path)\n",
    "\n",
    "    print(f\"Data preprocessing for {dataset_name} dataset...\")\n",
    "\n",
    "    # Preprocess and split the data\n",
    "    train_data, test_data, tune_data = preprocess_and_split(\n",
    "        data=data, seed=SEED, test_size=0.2, tune_size=0.1\n",
    "    )\n",
    "\n",
    "    print(f\"Creating dataloaders for {dataset_name} dataset...\")\n",
    "\n",
    "    # Create dataloaders using the adjusted function\n",
    "    train_loader, test_loader, tune_loader = create_dataloaders(\n",
    "        train_data, test_data, tune_data, batch_size=64, seed=SEED\n",
    "    )\n",
    "\n",
    "    # Store dataloaders\n",
    "    dataloaders[dataset_name] = {\n",
    "        'train': train_loader,\n",
    "        'test': test_loader,\n",
    "        'tuning': tune_loader\n",
    "    }\n",
    "\n",
    "    # Clear variables to free up memory\n",
    "    del data, train_data, test_data, tune_data\n",
    "    gc.collect()\n",
    "\n",
    "print(dataloaders.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting mixed_dataset_perturbation_5 Training Set:\n",
      "--- Sample 1 ---\n",
      "States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Actions Batch Shape: torch.Size([64])\n",
      "Rewards Batch Shape: torch.Size([64])\n",
      "Next States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Dones Batch Shape: torch.Size([64])\n",
      "States Batch Data Type: torch.float32\n",
      "Actions Batch Data Type: torch.int64\n",
      "Rewards Batch Data Type: torch.float32\n",
      "Next States Batch Data Type: torch.float32\n",
      "Dones Batch Data Type: torch.float32\n",
      "\n",
      "First Sample Details:\n",
      "First State Shape: torch.Size([3, 210, 160])\n",
      "First State Min/Max: 0.0000/0.8392\n",
      "First Action: 8\n",
      "First Reward: 0.0000\n",
      "First Next State Shape: torch.Size([3, 210, 160])\n",
      "First Done: 0.0\n",
      "\n",
      "\n",
      "Inspecting mixed_dataset_perturbation_5 Testing Set\n",
      "--- Sample 1 ---\n",
      "States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Actions Batch Shape: torch.Size([64])\n",
      "Rewards Batch Shape: torch.Size([64])\n",
      "Next States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Dones Batch Shape: torch.Size([64])\n",
      "States Batch Data Type: torch.float32\n",
      "Actions Batch Data Type: torch.int64\n",
      "Rewards Batch Data Type: torch.float32\n",
      "Next States Batch Data Type: torch.float32\n",
      "Dones Batch Data Type: torch.float32\n",
      "\n",
      "First Sample Details:\n",
      "First State Shape: torch.Size([3, 210, 160])\n",
      "First State Min/Max: 0.0000/0.8392\n",
      "First Action: 2\n",
      "First Reward: 0.0000\n",
      "First Next State Shape: torch.Size([3, 210, 160])\n",
      "First Done: 0.0\n",
      "\n",
      "\n",
      "Inspecting mixed_dataset_perturbation_5 Tuning Set\n",
      "--- Sample 1 ---\n",
      "States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Actions Batch Shape: torch.Size([64])\n",
      "Rewards Batch Shape: torch.Size([64])\n",
      "Next States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Dones Batch Shape: torch.Size([64])\n",
      "States Batch Data Type: torch.float32\n",
      "Actions Batch Data Type: torch.int64\n",
      "Rewards Batch Data Type: torch.float32\n",
      "Next States Batch Data Type: torch.float32\n",
      "Dones Batch Data Type: torch.float32\n",
      "\n",
      "First Sample Details:\n",
      "First State Shape: torch.Size([3, 210, 160])\n",
      "First State Min/Max: 0.0000/0.8392\n",
      "First Action: 17\n",
      "First Reward: 0.0000\n",
      "First Next State Shape: torch.Size([3, 210, 160])\n",
      "First Done: 0.0\n"
     ]
    }
   ],
   "source": [
    "# inspect dataset samples\n",
    "dataset_name_to_inspect = 'mixed_dataset_perturbation_5'\n",
    "\n",
    "# inspect the training set of the dataset\n",
    "print(f\"Inspecting {dataset_name_to_inspect} Training Set:\")\n",
    "inspect_dataset_sample(dataloaders[dataset_name_to_inspect]['train'])\n",
    "print(\"\\n\")\n",
    "\n",
    "# inspect the testing set of the dataset\n",
    "print(f\"Inspecting {dataset_name_to_inspect} Testing Set\")\n",
    "inspect_dataset_sample(dataloaders[dataset_name_to_inspect]['test'])\n",
    "print(\"\\n\")\n",
    "\n",
    "# inspect the tuning set of the dataset\n",
    "print(f\"Inspecting {dataset_name_to_inspect} Tuning Set\")\n",
    "inspect_dataset_sample(dataloaders[dataset_name_to_inspect]['tuning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### continue training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### continue training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### continue training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10% Perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mixed_dataset_perturbation_10 dataset...\n",
      "Data preprocessing for mixed_dataset_perturbation_10 dataset...\n",
      "Creating dataloaders for mixed_dataset_perturbation_10 dataset...\n",
      "dict_keys(['mixed_dataset_perturbation_10'])\n"
     ]
    }
   ],
   "source": [
    "# Paths to your datasets\n",
    "dataset_paths = [\n",
    "    'datasets/mixed/mixed_logs/mixed_dataset_perturbation_10.pkl',\n",
    "]\n",
    "\n",
    "dataloaders = {}  # Store dataloaders for each dataset\n",
    "\n",
    "for path in dataset_paths:\n",
    "    dataset_name = path.split('/')[-1].split('.')[0]  # Extract dataset name\n",
    "\n",
    "    print(f\"Loading {dataset_name} dataset...\")\n",
    "\n",
    "    # Load dataset\n",
    "    data = load_dataset(path)\n",
    "\n",
    "    print(f\"Data preprocessing for {dataset_name} dataset...\")\n",
    "\n",
    "    # Preprocess and split the data\n",
    "    train_data, test_data, tune_data = preprocess_and_split(\n",
    "        data=data, seed=SEED, test_size=0.2, tune_size=0.1\n",
    "    )\n",
    "\n",
    "    print(f\"Creating dataloaders for {dataset_name} dataset...\")\n",
    "\n",
    "    # Create dataloaders using the adjusted function\n",
    "    train_loader, test_loader, tune_loader = create_dataloaders(\n",
    "        train_data, test_data, tune_data, batch_size=64, seed=SEED\n",
    "    )\n",
    "\n",
    "    # Store dataloaders\n",
    "    dataloaders[dataset_name] = {\n",
    "        'train': train_loader,\n",
    "        'test': test_loader,\n",
    "        'tuning': tune_loader\n",
    "    }\n",
    "\n",
    "    # Clear variables to free up memory\n",
    "    del data, train_data, test_data, tune_data\n",
    "    gc.collect()\n",
    "\n",
    "print(dataloaders.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting mixed_dataset_perturbation_10 Training Set:\n",
      "--- Sample 1 ---\n",
      "States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Actions Batch Shape: torch.Size([64])\n",
      "Rewards Batch Shape: torch.Size([64])\n",
      "Next States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Dones Batch Shape: torch.Size([64])\n",
      "States Batch Data Type: torch.float32\n",
      "Actions Batch Data Type: torch.int64\n",
      "Rewards Batch Data Type: torch.float32\n",
      "Next States Batch Data Type: torch.float32\n",
      "Dones Batch Data Type: torch.float32\n",
      "\n",
      "First Sample Details:\n",
      "First State Shape: torch.Size([3, 210, 160])\n",
      "First State Min/Max: 0.0000/0.8392\n",
      "First Action: 15\n",
      "First Reward: 0.0000\n",
      "First Next State Shape: torch.Size([3, 210, 160])\n",
      "First Done: 0.0\n",
      "\n",
      "\n",
      "Inspecting mixed_dataset_perturbation_10 Testing Set\n",
      "--- Sample 1 ---\n",
      "States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Actions Batch Shape: torch.Size([64])\n",
      "Rewards Batch Shape: torch.Size([64])\n",
      "Next States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Dones Batch Shape: torch.Size([64])\n",
      "States Batch Data Type: torch.float32\n",
      "Actions Batch Data Type: torch.int64\n",
      "Rewards Batch Data Type: torch.float32\n",
      "Next States Batch Data Type: torch.float32\n",
      "Dones Batch Data Type: torch.float32\n",
      "\n",
      "First Sample Details:\n",
      "First State Shape: torch.Size([3, 210, 160])\n",
      "First State Min/Max: 0.0000/0.8392\n",
      "First Action: 13\n",
      "First Reward: 0.0000\n",
      "First Next State Shape: torch.Size([3, 210, 160])\n",
      "First Done: 0.0\n",
      "\n",
      "\n",
      "Inspecting mixed_dataset_perturbation_10 Tuning Set\n",
      "--- Sample 1 ---\n",
      "States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Actions Batch Shape: torch.Size([64])\n",
      "Rewards Batch Shape: torch.Size([64])\n",
      "Next States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Dones Batch Shape: torch.Size([64])\n",
      "States Batch Data Type: torch.float32\n",
      "Actions Batch Data Type: torch.int64\n",
      "Rewards Batch Data Type: torch.float32\n",
      "Next States Batch Data Type: torch.float32\n",
      "Dones Batch Data Type: torch.float32\n",
      "\n",
      "First Sample Details:\n",
      "First State Shape: torch.Size([3, 210, 160])\n",
      "First State Min/Max: 0.0000/0.8392\n",
      "First Action: 4\n",
      "First Reward: 0.0000\n",
      "First Next State Shape: torch.Size([3, 210, 160])\n",
      "First Done: 0.0\n"
     ]
    }
   ],
   "source": [
    "# inspect dataset samples\n",
    "dataset_name_to_inspect = 'mixed_dataset_perturbation_10'\n",
    "\n",
    "# inspect the training set of the dataset\n",
    "print(f\"Inspecting {dataset_name_to_inspect} Training Set:\")\n",
    "inspect_dataset_sample(dataloaders[dataset_name_to_inspect]['train'])\n",
    "print(\"\\n\")\n",
    "\n",
    "# inspect the testing set of the dataset\n",
    "print(f\"Inspecting {dataset_name_to_inspect} Testing Set\")\n",
    "inspect_dataset_sample(dataloaders[dataset_name_to_inspect]['test'])\n",
    "print(\"\\n\")\n",
    "\n",
    "# inspect the tuning set of the dataset\n",
    "print(f\"Inspecting {dataset_name_to_inspect} Tuning Set\")\n",
    "inspect_dataset_sample(dataloaders[dataset_name_to_inspect]['tuning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Replay Agents for BC, IQL and DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0% Perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading replay_dataset_perturbation_0 dataset...\n",
      "Data preprocessing for replay_dataset_perturbation_0 dataset...\n",
      "Creating dataloaders for replay_dataset_perturbation_0 dataset...\n",
      "dict_keys(['replay_dataset_perturbation_0'])\n"
     ]
    }
   ],
   "source": [
    "# Paths to your datasets\n",
    "dataset_paths = [\n",
    "    'datasets/replay/replay_logs/replay_dataset_perturbation_0.pkl',\n",
    "]\n",
    "\n",
    "dataloaders = {}  # Store dataloaders for each dataset\n",
    "\n",
    "for path in dataset_paths:\n",
    "    dataset_name = path.split('/')[-1].split('.')[0]  # Extract dataset name\n",
    "\n",
    "    print(f\"Loading {dataset_name} dataset...\")\n",
    "\n",
    "    # Load dataset\n",
    "    data = load_dataset(path)\n",
    "\n",
    "    print(f\"Data preprocessing for {dataset_name} dataset...\")\n",
    "\n",
    "    # Preprocess and split the data\n",
    "    train_data, test_data, tune_data = preprocess_and_split(\n",
    "        data=data, seed=SEED, test_size=0.2, tune_size=0.1\n",
    "    )\n",
    "\n",
    "    print(f\"Creating dataloaders for {dataset_name} dataset...\")\n",
    "\n",
    "    # Create dataloaders using the adjusted function\n",
    "    train_loader, test_loader, tune_loader = create_dataloaders(\n",
    "        train_data, test_data, tune_data, batch_size=64, seed=SEED\n",
    "    )\n",
    "\n",
    "    # Store dataloaders\n",
    "    dataloaders[dataset_name] = {\n",
    "        'train': train_loader,\n",
    "        'test': test_loader,\n",
    "        'tuning': tune_loader\n",
    "    }\n",
    "\n",
    "    # Clear variables to free up memory\n",
    "    del data, train_data, test_data, tune_data\n",
    "    gc.collect()\n",
    "\n",
    "print(dataloaders.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting replay_dataset_perturbation_0 Training Set:\n",
      "--- Sample 1 ---\n",
      "States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Actions Batch Shape: torch.Size([64])\n",
      "Rewards Batch Shape: torch.Size([64])\n",
      "Next States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Dones Batch Shape: torch.Size([64])\n",
      "States Batch Data Type: torch.float32\n",
      "Actions Batch Data Type: torch.int64\n",
      "Rewards Batch Data Type: torch.float32\n",
      "Next States Batch Data Type: torch.float32\n",
      "Dones Batch Data Type: torch.float32\n",
      "\n",
      "First Sample Details:\n",
      "First State Shape: torch.Size([3, 210, 160])\n",
      "First State Min/Max: 0.0000/0.8392\n",
      "First Action: 9\n",
      "First Reward: 0.0000\n",
      "First Next State Shape: torch.Size([3, 210, 160])\n",
      "First Done: 0.0\n",
      "\n",
      "\n",
      "Inspecting replay_dataset_perturbation_0 Testing Set\n",
      "--- Sample 1 ---\n",
      "States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Actions Batch Shape: torch.Size([64])\n",
      "Rewards Batch Shape: torch.Size([64])\n",
      "Next States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Dones Batch Shape: torch.Size([64])\n",
      "States Batch Data Type: torch.float32\n",
      "Actions Batch Data Type: torch.int64\n",
      "Rewards Batch Data Type: torch.float32\n",
      "Next States Batch Data Type: torch.float32\n",
      "Dones Batch Data Type: torch.float32\n",
      "\n",
      "First Sample Details:\n",
      "First State Shape: torch.Size([3, 210, 160])\n",
      "First State Min/Max: 0.0000/0.8392\n",
      "First Action: 2\n",
      "First Reward: 0.0000\n",
      "First Next State Shape: torch.Size([3, 210, 160])\n",
      "First Done: 0.0\n",
      "\n",
      "\n",
      "Inspecting replay_dataset_perturbation_0 Tuning Set\n",
      "--- Sample 1 ---\n",
      "States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Actions Batch Shape: torch.Size([64])\n",
      "Rewards Batch Shape: torch.Size([64])\n",
      "Next States Batch Shape: torch.Size([64, 3, 210, 160])\n",
      "Dones Batch Shape: torch.Size([64])\n",
      "States Batch Data Type: torch.float32\n",
      "Actions Batch Data Type: torch.int64\n",
      "Rewards Batch Data Type: torch.float32\n",
      "Next States Batch Data Type: torch.float32\n",
      "Dones Batch Data Type: torch.float32\n",
      "\n",
      "First Sample Details:\n",
      "First State Shape: torch.Size([3, 210, 160])\n",
      "First State Min/Max: 0.0000/0.9373\n",
      "First Action: 12\n",
      "First Reward: 0.0000\n",
      "First Next State Shape: torch.Size([3, 210, 160])\n",
      "First Done: 0.0\n"
     ]
    }
   ],
   "source": [
    "# inspect dataset samples\n",
    "dataset_name_to_inspect = 'replay_dataset_perturbation_0'\n",
    "\n",
    "# inspect the training set of the dataset\n",
    "print(f\"Inspecting {dataset_name_to_inspect} Training Set:\")\n",
    "inspect_dataset_sample(dataloaders[dataset_name_to_inspect]['train'])\n",
    "print(\"\\n\")\n",
    "\n",
    "# inspect the testing set of the dataset\n",
    "print(f\"Inspecting {dataset_name_to_inspect} Testing Set\")\n",
    "inspect_dataset_sample(dataloaders[dataset_name_to_inspect]['test'])\n",
    "print(\"\\n\")\n",
    "\n",
    "# inspect the tuning set of the dataset\n",
    "print(f\"Inspecting {dataset_name_to_inspect} Tuning Set\")\n",
    "inspect_dataset_sample(dataloaders[dataset_name_to_inspect]['tuning'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master-thesis-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
